<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Javier  Cabrera Arteaga | Porting your ML model as an onnxruntime WebAssembly application (pseudo-tutorial)</title>
<meta name="description" content="I ported a keras ML model to a fully isolated inferring application in WebAssembly. It runs completely in the browser. To do so, I use Emscripten, the onnxruntime and a few lines of Rust code.
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2023/ONNx/">

<!-- Open Graph -->

<meta property="og:site_name" content="Personal blog
I ported a keras ML model to a fully isolated inferring application in WebAssembly. It runs completely in the browser. To do so, I use Emscripten, the onnxruntime and a few lines of Rust code.
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="Porting your ML model as an onnxruntime WebAssembly application (pseudo-tutorial)" />
<meta property="og:url" content="/blog/2023/ONNx/" />
<meta name="description" property="og:description" content="I ported a keras ML model to a fully isolated inferring application in WebAssembly. It runs completely in the browser. To do so, I use Emscripten, the onnxruntime and a few lines of Rust code.
" />
<meta name="author" content="Javier Cabrera-Arteaga">
<meta name="image" property="og:image" content="/assets/img/inferring.png" />


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Javier</span>   Cabrera Arteaga PhD 
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/cv/">
                CV
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/thesis/">
                Lic Thesis
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/pthesis/">
                PhD Thesis
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Porting your ML model as an onnxruntime WebAssembly application (pseudo-tutorial)</h1>
    <p class="post-meta">December 6, 2023 • WebAssembly, ONNX, ML, MLOps
</p>
  </header>

  <article class="post-content">
    <p>About a year ago, we needed to <a href="https://github.com/Jacarte/ralph">reproduce an experiment to detect cryptomalware</a>. The experiment we replicated was <a href="https://www.ndss-symposium.org/ndss-paper/minos-a-lightweight-real-time-cryptojacking-detection-system/">MINOS</a>, a neural network solution for the rapid detection of WebAssembly cryptomalware. The concept behind MINOS is to receive a binary bytestream as input, and then transform it into a 100x100 grayscale pixel image. Once the image is generated, a neural network provides a <code class="language-plaintext highlighter-rouge">BENIGN</code> or <code class="language-plaintext highlighter-rouge">MALIGN</code> classification over the 100x100 pixels as features. This method, previously used to detect malicious dll in Windows, was particularly novel in the context of WebAssembly. This is especially true since this method can classify binaries at an impressive speed. It is important to note that malware detection is a considerably difficult problem.</p>

<blockquote>
  <p>Speaking anecdotally, we reproduced to emphasize the impact of obfuscation in disrupting these types of methods, <a href="https://www.sciencedirect.com/science/article/pii/S0167404823002067">particularly for WebAssembly</a>.</p>
</blockquote>

<p>I received contact from colleagues regarding the sharing of our reproduction. 
At that time, we responded positively due to our advocacy for open-science and reproduction (we always provide a publicly available repository). 
We dispatched a Jupyter notebook illustrating the training and usage of the MINOS model. 
Additionally, we shared the h5 file. 
So, in theory, one could simply reopen the model and infer a new passed binary, as demonstrated in the following snippet.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="rouge-code"><pre>    <span class="p">...</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s">"minos.h5"</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="p">...</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="s">"""
        Given dataframe, uses the model to
        predict the labels

        Parameters
        ----------
        data : pandas.DataFrame
            The dataset frame containing the instances to be predicted
        """</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">d</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">MINOS</span><span class="p">.</span><span class="n">classes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">d</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>However, there was something in my thoughts: <em>there must exist a more efficient method to disseminate, at the very least, the inference process</em>. Using the provided code snippet above, you can establish your own <code class="language-plaintext highlighter-rouge">flask</code> server API and expedite the inference process significantly. This method, however, suffers from inadequate scalability. What if the goal is to deploy this in a production environment? Several solutions exist that address the automatic scaling of such a service. <strong>Yet, my interest lay in investigating a zero-cost solution. Is it possible to execute the model for inference directly in the browser?</strong>
Imagine a design-sketch website that uses AI and purely migrates the AI inferring to your browser.</p>

<p>A solution like this could potentially solve many problems.</p>
<ul>
  <li>Users will not need to install anything (no <code class="language-plaintext highlighter-rouge">pip install -r requirements.txt</code>, thanks). 
Users can simply open a webpage and execute the inference with their browser. 
After all, a web browser is essentially a virtual machine.</li>
  <li>This solution is cost-effective; it could be deployed on a static webpage, bypassing the need for a machine learning inference pipeline infrastructure. 
Consider, for instance, that the demo page is hosted by GitHub static pages system.</li>
  <li>In addition, it would not need internet. Once the webpage is completely loaded, it would run without making external query. 
Imagine a <a href="https://en.wikipedia.org/wiki/Progressive_web_app">PWA application</a> with this.</li>
  <li>Last but not least, by ensuring that data remains confined to the browser during the inference process, this method not only enhances safety but also contributes significantly to safeguarding user privacy.</li>
</ul>

<p>The subsequent text describes how I succeeded in doing so. However, if you want to try it immediately, you may access the <a href="https://www.jacarte.me/ralph/">demo</a>. Here, https://github.com/Jacarte/ralph, you can find the repository.</p>

<h1 id="the-how">The how</h1>

<p>My initial attempt focused on discovering a method to directly execute the h5 file. 
Unfortunately, this effort proved unsuccessful as I was unable to locate a viable solution (or one that I liked it). 
Yet, during the process, I learned about <a href="https://onnx.ai/">onnx</a>.</p>

<blockquote>
  <p>ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers</p>
</blockquote>

<p>The PyTorch team at Facebook originally developed ONNX under the name Toffee. 
In September 2017, Facebook and Microsoft announced a renaming of Toffee to ONNX. 
The community also introduced a method to deploy machine learning models using this format. 
The <a href="https://onnxruntime.ai/">onxxruntime</a> project, primarily implemented in C++, proves to be exceptionally fast when using an ONNX file for inference. 
Most significantly, the <a href="https://onnxruntime.ai/docs/build/web.html"><strong>onxxruntime can be compiled to WebAssembly</strong></a>. 
WebAssembly serves as the ideal solution for porting such projects to the web. 
It surpasses JavaScript in performance within the browser, notably on mobile devices. 
The Emscripten compiler is used to port the onnxruntime to WebAssembly.
In essence, I just needed to find a method to convert the h5 file of the model to an ONNX file, and then employ the onxxruntime to execute it in the browser.</p>

<p>It requires two lines of Python code to generate an ONNX file from a Keras model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">tf2onnx</span>
<span class="n">model_proto</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf2onnx</span><span class="p">.</span><span class="n">convert</span><span class="p">.</span><span class="n">from_keras</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">,</span>  <span class="n">opset</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="n">model_name</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>I had all necessary components: a runtime ported to Wasm to load and use an ML model. I only required a few lines of JavaScript to ensure the MVP. However, the issue with this current solution is its distribution. I would inevitably end up disseminating the model and the inferring runtime as separate artifacts - the Wasm runtime on one side and the onnx file model on the other. 
Besides, in any case the input needs to be turned into a “grayscale” image before feeding the model.
<strong>To solve this, I chose to package all into a single WebAssembly file: the model, the runtime and the preprocessing of the input binary as a grayscale image.</strong> 
Ideally, the application would contain a single entry point - the vector used as input for inferring, in our case, the bytestream of potential malware.</p>

<blockquote>
  <p>Disclaimer: The onnxruntime project provides <a href="https://onnxruntime.ai/docs/tutorials/web/">tutorials</a> aimed at achieving the goal of porting inference to the web. Yet, note that in the existing tutorials, the data preprocessing is not handled within the WebAssembly binary. The key distinction in my approach lies in precisely encapsulating all components within the same Wasm program, which is expected to result in improved performance and efficiency.</p>
</blockquote>

<p>To the best of my knowledge, Rust stands as the premier toolchain for compiling WebAssembly binaries. Though Emscripten is great, it is afflicted by typical C/C++ project issues, e.g., the configuration for compiling C/C++ projects is inherently challenging. Fortunately, a solution to this problem exists in the form of onnxruntime bindings in Rust. The <a href="https://github.com/pykeio/ort">ort</a> project serves as a Rust wrapper for onnxruntime projects. Using it allows us to produce a single WebAssembly file that includes the onnx model, the runtime and the preprocessing logic of the input.</p>

<p>The following snippet illustrates the application’s code.
The application embeds the model on its third line of code, achieved simply by injecting the model file as pure bytes.
The <code class="language-plaintext highlighter-rouge">init_model</code> function loads the model and assembles the inference pipeline.
The <code class="language-plaintext highlighter-rouge">infer</code> function takes a pointer in memory and the size of the byestream, converts it into an image, and feeds the model to obtain the classification.</p>

<blockquote>
  <p>Notice this code is just an MVP. I am sure there exist better ways to implement such code. To begin with, the <code class="language-plaintext highlighter-rouge">unwrap</code>ing is definitely a bad practice.</p>
</blockquote>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="c1"># Cargo.toml</span>
<span class="pi">[</span><span class="nv">package</span><span class="pi">]</span>
<span class="s">name = "wasm_wrapper"</span>
<span class="s">version = "0.1.0"</span>
<span class="s">edition = "2021"</span>
<span class="s">description = "Simple wasm wrapper for an ONNX model"</span>

<span class="pi">[</span><span class="nv">lib</span><span class="pi">]</span>
<span class="s">crate-type = ["cdylib"]</span>

<span class="pi">[</span><span class="nv">dependencies</span><span class="pi">]</span>
<span class="s">ort = "2.0.0-alpha.2"</span>
<span class="s">image = "0.24.7"</span>
<span class="s">ndarray = "0.15"</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
</pre></td><td class="rouge-code"><pre><span class="c1">// main.rs</span>
<span class="k">use</span> <span class="nn">ort</span><span class="p">::{</span><span class="n">inputs</span><span class="p">,</span> <span class="n">GraphOptimizationLevel</span><span class="p">,</span> <span class="n">Session</span><span class="p">,</span> <span class="n">SessionOutputs</span><span class="p">};</span>
<span class="k">use</span> <span class="nn">ndarray</span><span class="p">::{</span><span class="n">Array</span><span class="p">,</span> <span class="n">Array1</span><span class="p">};</span>

<span class="c1">// Embed the model here</span>
<span class="k">static</span> <span class="n">MODEL</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">u8</span><span class="p">]</span> <span class="o">=</span> <span class="nd">include_bytes!</span><span class="p">(</span><span class="s">"model.onnx"</span><span class="p">);</span>

<span class="c1">// Load the model in the start function</span>
<span class="k">static</span> <span class="k">mut</span> <span class="n">model</span><span class="p">:</span> <span class="nb">Option</span><span class="o">&lt;</span><span class="nn">ort</span><span class="p">::</span><span class="n">Session</span><span class="o">&gt;</span> <span class="o">=</span> <span class="nb">None</span><span class="p">;</span>

<span class="c1">// FIXME: this should be optimized with Wizer, in order to avoid the loading of the data on every fresh</span>
<span class="c1">// spawn of a potential wasm32-wasi module :)</span>
<span class="nd">#[no_mangle]</span>
<span class="k">pub</span> <span class="k">extern</span> <span class="s">"C"</span> <span class="k">fn</span> <span class="nf">init_model</span><span class="p">(){</span>
    <span class="k">unsafe</span>  <span class="p">{</span>
        <span class="nd">println!</span><span class="p">(</span><span class="s">"Init model...!"</span><span class="p">);</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nf">Some</span><span class="p">(</span>
        <span class="nn">ort</span><span class="p">::</span><span class="nn">Session</span><span class="p">::</span><span class="nf">builder</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">()</span>
            <span class="nf">.with_optimization_level</span><span class="p">(</span><span class="nn">GraphOptimizationLevel</span><span class="p">::</span><span class="n">Level3</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">()</span>
            <span class="c1">// Sadly we cannot add several threads here due to that this will be loaded in a single Wasm thread</span>
            <span class="nf">.with_model_from_memory</span><span class="p">(</span><span class="o">&amp;</span><span class="n">MODEL</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">()</span>
        <span class="p">);</span>
        <span class="nd">println!</span><span class="p">(</span><span class="s">"Model ready...!"</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>


<span class="nd">#[no_mangle]</span>
<span class="k">pub</span> <span class="k">extern</span> <span class="s">"C"</span> <span class="k">fn</span> <span class="nf">infer</span><span class="p">(</span><span class="n">wasm_ptr</span><span class="p">:</span> <span class="o">*</span><span class="k">const</span> <span class="nb">u8</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">usize</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nb">i32</span>  <span class="p">{</span>
    <span class="c1">// Turn the bytes into a vector of 100x100 integers</span>
    <span class="k">let</span> <span class="n">wasm_bytes</span> <span class="o">=</span> <span class="k">unsafe</span> <span class="p">{</span> <span class="nn">std</span><span class="p">::</span><span class="nn">slice</span><span class="p">::</span><span class="nf">from_raw_parts</span><span class="p">(</span><span class="n">wasm_ptr</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="p">};</span>
    <span class="k">let</span> <span class="n">sqrt</span> <span class="o">=</span> <span class="p">(</span><span class="n">wasm_bytes</span><span class="nf">.len</span><span class="p">()</span> <span class="k">as</span> <span class="nb">f64</span><span class="p">)</span><span class="nf">.sqrt</span><span class="p">()</span> <span class="k">as</span> <span class="nb">usize</span><span class="p">;</span>
    <span class="k">if</span> <span class="n">sqrt</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">{</span>
        <span class="nd">println!</span><span class="p">(</span><span class="s">"Invalid input len 0"</span><span class="p">);</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Create an image from the bytes using sqrt*sqrt size</span>
    <span class="c1">// It is grayscale, so we can use a single channel</span>
    <span class="k">let</span> <span class="n">img</span> <span class="o">=</span> <span class="nn">image</span><span class="p">::</span><span class="nn">GrayImage</span><span class="p">::</span><span class="nf">from_raw</span><span class="p">(</span><span class="n">sqrt</span> <span class="k">as</span> <span class="nb">u32</span><span class="p">,</span> <span class="n">sqrt</span> <span class="k">as</span> <span class="nb">u32</span><span class="p">,</span> <span class="n">wasm_bytes</span><span class="nf">.to_vec</span><span class="p">())</span><span class="nf">.unwrap</span><span class="p">();</span>
    <span class="c1">// Now scale it to 100x100</span>
    <span class="k">let</span> <span class="n">img</span> <span class="o">=</span> <span class="nn">image</span><span class="p">::</span><span class="nn">imageops</span><span class="p">::</span><span class="nf">resize</span><span class="p">(</span><span class="o">&amp;</span><span class="n">img</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="nn">image</span><span class="p">::</span><span class="nn">imageops</span><span class="p">::</span><span class="nn">FilterType</span><span class="p">::</span><span class="n">Nearest</span><span class="p">);</span>
    <span class="c1">// Convert it to a vector of floats</span>
    <span class="k">let</span> <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="nf">.into_raw</span><span class="p">()</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.map</span><span class="p">(|</span><span class="n">x</span><span class="p">|</span> <span class="o">*</span><span class="n">x</span> <span class="k">as</span> <span class="nb">f32</span><span class="p">)</span><span class="py">.collect</span><span class="p">::</span><span class="o">&lt;</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;&gt;</span><span class="p">();</span>

    <span class="c1">// Call the inferring</span>
    <span class="k">if</span> <span class="k">unsafe</span> <span class="p">{</span> <span class="n">model</span><span class="nf">.is_none</span><span class="p">()</span> <span class="p">}</span> <span class="p">{</span>
        <span class="nd">println!</span><span class="p">(</span><span class="s">"Model not initialized"</span><span class="p">);</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="nd">println!</span><span class="p">(</span><span class="s">"Feeding input"</span><span class="p">);</span>
    
    <span class="k">let</span> <span class="k">mut</span> <span class="n">input</span> <span class="o">=</span> <span class="nn">Array</span><span class="p">::</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="o">*</span><span class="mi">100</span><span class="p">));</span>
    <span class="n">input</span><span class="nf">.assign</span><span class="p">(</span><span class="o">&amp;</span><span class="nn">Array</span><span class="p">::</span><span class="nf">from_shape_vec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="o">*</span><span class="mi">100</span><span class="p">),</span> <span class="n">img</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">());</span>

    <span class="k">let</span> <span class="n">input</span> <span class="o">=</span> <span class="nd">inputs!</span><span class="p">[</span><span class="s">"reshape_input"</span> <span class="k">=&gt;</span> <span class="n">input</span><span class="nf">.view</span><span class="p">()]</span><span class="nf">.unwrap</span><span class="p">();</span>
    <span class="k">let</span> <span class="n">modelref</span> <span class="o">=</span> <span class="k">unsafe</span> <span class="p">{</span> <span class="n">model</span><span class="nf">.as_ref</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">()</span> <span class="p">};</span>    
    <span class="k">let</span> <span class="n">output</span> <span class="o">=</span> <span class="n">modelref</span><span class="nf">.run</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>

    <span class="k">match</span> <span class="n">output</span> <span class="p">{</span>
        <span class="nf">Ok</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="p">{</span>
            <span class="k">let</span> <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s">"dense"</span><span class="p">]</span><span class="py">.extract_tensor</span><span class="p">::</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">()</span>
                <span class="nf">.view</span><span class="p">()</span>
                <span class="nf">.t</span><span class="p">()</span> <span class="c1">// transpose</span>
                <span class="nf">.into_owned</span><span class="p">();</span>
            
            <span class="k">let</span> <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.map</span><span class="p">(|</span><span class="n">x</span><span class="p">|</span> <span class="o">*</span><span class="n">x</span> <span class="k">as</span> <span class="nb">f64</span><span class="p">)</span><span class="py">.collect</span><span class="p">::</span><span class="o">&lt;</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f64</span><span class="o">&gt;&gt;</span><span class="p">();</span>

            <span class="k">if</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="p">{</span>
                <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
            <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
            <span class="p">}</span>
        <span class="p">},</span>
        <span class="nf">Err</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="p">{</span>
            <span class="nd">println!</span><span class="p">(</span><span class="s">"Error: {:?}"</span><span class="p">,</span> <span class="n">e</span><span class="p">);</span>
            <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>I compiled the application with the command below. This command establishes several parameters for Emscripten and constructs the application in the <code class="language-plaintext highlighter-rouge">wasm32-unknown-emscripten</code> architecture.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nv">EMCC_CFLAGS</span><span class="o">=</span><span class="s2">"-sERROR_ON_UNDEFINED_SYMBOLS=0 -s TOTAL_STACK=32MB -s ASSERTIONS=2 -s TOTAL_MEMORY=256MB -s ALLOW_MEMORY_GROWTH=1 -sEXPORTED_FUNCTIONS=</span><span class="se">\"</span><span class="s2">['_malloc', '_infer', '_init_model']</span><span class="se">\"</span><span class="s2"> --minify 0 -Os -sMODULARIZE=1 -o dist/model.mjs"</span> cargo build <span class="nt">--target</span><span class="o">=</span>wasm32-unknown-emscripten
</pre></td></tr></tbody></table></code></pre></div></div>

<p>As a result we have the following two files as the application:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>9.5Mb model.wasm
200Kb model.mjs
</pre></td></tr></tbody></table></code></pre></div></div>

<p>To use them in a webpage, I needed just a few lines of code.</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;script </span><span class="na">type=</span><span class="s">"module"</span><span class="nt">&gt;</span>
        <span class="k">import</span> <span class="nx">model</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">./model.mjs</span><span class="dl">'</span><span class="p">;</span>
        <span class="nx">model</span><span class="p">().</span><span class="nx">then</span><span class="p">(</span><span class="nx">r</span> <span class="o">=&gt;</span> <span class="p">{</span>
            <span class="nx">r</span><span class="p">.</span><span class="nx">_init_model</span><span class="p">();</span>

            <span class="c1">// Set the callback for loading a file in the form</span>
            <span class="nb">document</span><span class="p">.</span><span class="nx">getElementById</span><span class="p">(</span><span class="dl">'</span><span class="s1">file</span><span class="dl">'</span><span class="p">).</span><span class="nx">addEventListener</span><span class="p">(</span><span class="dl">'</span><span class="s1">change</span><span class="dl">'</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">e</span><span class="p">)</span> <span class="p">{</span>
                <span class="kd">var</span> <span class="nx">file</span> <span class="o">=</span> <span class="nx">e</span><span class="p">.</span><span class="nx">target</span><span class="p">.</span><span class="nx">files</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
                <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="nx">file</span><span class="p">)</span> <span class="p">{</span>
                    <span class="k">return</span><span class="p">;</span>
                <span class="p">}</span>
                <span class="kd">var</span> <span class="nx">reader</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">FileReader</span><span class="p">();</span>
                <span class="nx">reader</span><span class="p">.</span><span class="nx">onload</span> <span class="o">=</span> <span class="kd">function</span><span class="p">(</span><span class="nx">e</span><span class="p">)</span> <span class="p">{</span>
                    <span class="kd">var</span> <span class="nx">contents</span> <span class="o">=</span> <span class="nx">e</span><span class="p">.</span><span class="nx">target</span><span class="p">.</span><span class="nx">result</span><span class="p">;</span>
                    <span class="kd">var</span> <span class="nx">content_bytes</span> <span class="o">=</span> <span class="k">new</span> <span class="nb">Uint8Array</span><span class="p">(</span><span class="nx">contents</span><span class="p">);</span>
                    <span class="c1">// We need to send a ptr</span>
                    <span class="kd">var</span> <span class="nx">input_ptr</span>  <span class="o">=</span> <span class="nx">r</span><span class="p">.</span><span class="nx">_malloc</span><span class="p">(</span><span class="nx">content_bytes</span><span class="p">.</span><span class="nx">length</span><span class="p">);</span>
                    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">input_ptr</span><span class="p">);</span>
                    <span class="nx">r</span><span class="p">.</span><span class="nx">HEAPU8</span><span class="p">.</span><span class="kd">set</span><span class="p">(</span><span class="nx">content_bytes</span><span class="p">,</span> <span class="nx">input_ptr</span><span class="p">);</span> 
                    <span class="c1">// write WASM memory calling the set method for the Uint8Array</span>

                    <span class="kd">let</span> <span class="nx">now</span> <span class="o">=</span> <span class="nx">performance</span><span class="p">.</span><span class="nx">now</span><span class="p">();</span>
                    <span class="kd">let</span> <span class="nx">result</span> <span class="o">=</span> <span class="nx">r</span><span class="p">.</span><span class="nx">_infer</span><span class="p">(</span><span class="nx">input_ptr</span><span class="p">,</span> <span class="nx">content_bytes</span><span class="p">.</span><span class="nx">length</span><span class="p">);</span>
                    <span class="nx">r</span><span class="p">.</span><span class="nx">_free</span><span class="p">(</span><span class="nx">input_ptr</span><span class="p">);</span>
                    <span class="nx">result</span> <span class="o">=</span> <span class="nx">result</span><span class="p">?</span> <span class="dl">"</span><span class="s2">BENIGN</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">MALICIOUS</span><span class="dl">"</span><span class="p">;</span>
                    <span class="nb">document</span><span class="p">.</span><span class="nx">getElementById</span><span class="p">(</span><span class="dl">'</span><span class="s1">val</span><span class="dl">'</span><span class="p">).</span><span class="nx">innerHTML</span> <span class="o">=</span> <span class="nx">result</span><span class="p">;</span>
                <span class="p">};</span>
                <span class="nx">reader</span><span class="p">.</span><span class="nx">readAsArrayBuffer</span><span class="p">(</span><span class="nx">file</span><span class="p">);</span>
            <span class="p">});</span>
        <span class="p">}).</span><span class="k">catch</span><span class="p">(</span><span class="nx">r</span> <span class="o">=&gt;</span> <span class="p">{</span>
        <span class="p">})</span>
    <span class="nt">&lt;/script&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>See the live demo <a href="https://www.jacarte.me/ralph/">here</a>. 
This represents zero-cost ML model deployment running in your browser. 
The performance numbers are quite impressive. 
On my computer, it takes nearly 30ms to infer from a 2Mb file. 
Please note, this time not only includes the inference but also the conversion of the binary into a grayscale image (check the gif below).</p>

<p><img src="/assets/img/demo.gif" alt="drawing" style="width: 100%" /></p>

<h1 id="discussion">Discussion</h1>

<p>I avoid drawing comparisons between this approach and others. I leave this task to you. In lieu of this, I will outline two specific points I have observed throughout the process of implementing this.</p>

<p>First, it must be acknowledged that this method is not foolproof. If the model size is on the larger side, a webpage may require several minutes simply to render the model available for execution. However, the capability of the Rust wrapper to compile to Wasm makes it an apt choice for Cloudflare workers or Fastly Edge@Compute. This allows for the transfer of inference to the Edge. Such a transition boasts additional benefits, including the automatic management of scalability. It’s worth noting that launching a Wasm program in the Edge takes nothing more than a few nanoseconds.</p>

<p>Second, the necessity for manual implementation of the interaction with the host browser is a notable drawback. Take, for instance, the <code class="language-plaintext highlighter-rouge">infer</code> function in the Rust code, which requires a pointer.
Passing this pointer takes 3 lines of JavaScript code at least (take a look to the last snippet). 
Ideally, it should only need to handle a byte array type, something akin to <code class="language-plaintext highlighter-rouge">wasm: &amp;[u8]</code>. There exists a solution: <a href="https://rustwasm.github.io/docs/wasm-bindgen/print.html"><code class="language-plaintext highlighter-rouge">wasm-bindgen</code></a>. However, this only functions for the <code class="language-plaintext highlighter-rouge">wasm32-wasi</code> architecture. Regrettably, the onnxruntime has a significant dependency on Emscripten, i.e., needing compilation via Emscripten. This presents further issues with potential Edge deployment, as Wasm-WASI programs are typically more suitable for running in the Edge. On a positive note, people are working on a promising alternative to load ML models and operate them for inferring in WebAssembly, <a href="https://github.com/deislabs/wasi-nn-onnx">the WASI-NN proposal</a>.</p>


  </article>

  
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'jacarte-github-io';
      var disqus_identifier = '/blog/2023/ONNx';
      var disqus_title      = "Porting your ML model as an onnxruntime WebAssembly application (pseudo-tutorial)";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2024 Javier  Cabrera Arteaga.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha512-/DXTXr6nQodMUiq+IUJYCt2PPOUjrHJ9wFrqpJ3XkgPNOZVfMok7cRw6CSxyCQxXn6ozlESsSh1/sMCTF1rL/g==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js"  integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>



<!-- Enable Tooltips -->
<script type="text/javascript">
$(function () {
  $('[data-toggle="tooltip"]').tooltip()
})
</script>



<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-127898106-1', 'auto');
ga('send', 'pageview');
</script>



</html>
